{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['https_proxy'] = \"http://hpc-proxy00.city.ac.uk:3128\"\n",
    "# os.environ['http_proxy'] = “http://hpc-proxy00.city.ac.uk:3128”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),     \n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='.'\n",
    "filedir=os.path.join(os.getcwd(),filepath,'Vegetable Images')\n",
    "batch_size=128\n",
    "\n",
    "for i in os.listdir(filedir):\n",
    "    if(i=='train'):\n",
    "        train_dataset=datasets.ImageFolder(\n",
    "            root=os.path.join(filedir,i),\n",
    "            transform=transform_train\n",
    "    )\n",
    "    elif(i=='test'):\n",
    "        test_dataset=datasets.ImageFolder(\n",
    "            root=os.path.join(filedir,i),\n",
    "            transform=transform_test\n",
    "    )\n",
    "    elif(i=='validation'):\n",
    "        valid_dataset=datasets.ImageFolder(\n",
    "            root=os.path.join(filedir,i),\n",
    "            transform=transform_test\n",
    "    )\n",
    "    else:\n",
    "        raise(Exception('Unexpected error occurred.'))\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=True)\n",
    "valid_loader=DataLoader(valid_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1=16\n",
    "D2=32\n",
    "D3=64\n",
    "D4=128\n",
    "D5=256\n",
    "\n",
    "A1=512\n",
    "A2=256\n",
    "num_classes=15\n",
    "input_pix=224\n",
    "num_neurons=int(np.floor(np.floor(np.floor(input_pix/2)/2)/2)**2*D5)\n",
    "input=3\n",
    "class my_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(input, D1, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(D1,D2, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(D2, D3, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(D3 ,D4, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(D4, D5, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(D5 ,D5, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(num_neurons,A1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(A1, A2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(A2,num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "opt=torch.optim.Adam\n",
    "lr=1e-3\n",
    "model=my_nn().to(device)\n",
    "\n",
    "def train(model,train_loader,epochs,lr,opt,loss_func):\n",
    "    model.train()\n",
    "    losslog=[]\n",
    "    optimizer=opt(model.parameters(),lr)\n",
    "    for _,(x,y) in enumerate(train_loader):\n",
    "        batch_x=x.to(device)\n",
    "        ypred=model(batch_x)\n",
    "        loss=loss_func(ypred,y.to(device))\n",
    "        losslog.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return losslog\n",
    "\n",
    "def evaluate(model,test_loader):\n",
    "    model.eval()\n",
    "    batch_loss=[]\n",
    "    tally=0\n",
    "    total=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _,(x,y) in enumerate(test_loader):\n",
    "            ypred=model(x.to(device))\n",
    "            tally+=torch.tensor(torch.sum(ypred.argmax(1)==y.to(device)))\n",
    "            total+=len(ypred)\n",
    "            batch_loss.append(torch.tensor(torch.sum(ypred.argmax(1)==y.to(device)))/len(ypred))\n",
    "    \n",
    "    return tally/total\n",
    "            \n",
    "\n",
    "def fit(model,train_loader,test_loader,val_loader,epochs,lr=1e-3,opt=torch.optim.SGD,loss_func=F.cross_entropy):\n",
    "    train_acc_log=[]\n",
    "    test_acc_log=[]\n",
    "    valid_acc_log=[]\n",
    "    for iter in range(epochs):\n",
    "        #training step\n",
    "        train_log=train(model,train_loader,epochs,lr,opt,loss_func)\n",
    "            \n",
    "        train_acc=evaluate(model,train_loader)\n",
    "        test_acc=evaluate(model,test_loader)\n",
    "        valid_acc=evaluate(model,test_loader)\n",
    "        print(\"Epoch [{}], train acc: {:.2f}, test acc: {:.2f}, val acc: {:.2f}\".format(epochs, train_acc, test_acc, valid_acc))\n",
    "        #evaluate step\n",
    "        train_acc_log.append(train_acc)\n",
    "        test_acc_log.append(test_acc)\n",
    "        valid_acc_log.append(valid_acc)\n",
    "    \n",
    "    print('Training Complete')\n",
    "    return train_acc,test_acc,valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_acc,test_acc,valid_acc\u001b[38;5;241m=\u001b[39mfit(model,train_loader,test_loader,valid_loader,\u001b[38;5;241m20\u001b[39m,opt\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam)\n",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, test_loader, val_loader, epochs, lr, opt, loss_func)\u001b[0m\n\u001b[0;32m     42\u001b[0m valid_acc_log\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#training step\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     train_log\u001b[38;5;241m=\u001b[39mtrain(model,train_loader,epochs,lr,opt,loss_func)\n\u001b[0;32m     47\u001b[0m     train_acc\u001b[38;5;241m=\u001b[39mevaluate(model,train_loader)\n\u001b[0;32m     48\u001b[0m     test_acc\u001b[38;5;241m=\u001b[39mevaluate(model,test_loader)\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, epochs, lr, opt, loss_func)\u001b[0m\n\u001b[0;32m     16\u001b[0m     losslog\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losslog\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc,test_acc,valid_acc=fit(model,train_loader,test_loader,valid_loader,20,opt=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
